{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74267435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21558311",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = load_diabetes(return_X_y = True) # y default loads scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bef183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ebbe6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45084673378726603\n",
      "[  51.5573825  -139.35700348  353.5284863   259.73716396    1.94494252\n",
      "  -49.4915564  -166.58516249  138.21453748  317.60987748  106.05543454] [161.48041569]\n"
     ]
    }
   ],
   "source": [
    "# using Gradient descent to fit the ridge model \n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd = SGDRegressor(eta0 = 0.0266, learning_rate = 'constant', penalty = 'l2', max_iter = 100, alpha = 0.001) #eta0 is the learning_rate value\n",
    "sgd.fit(xtrain, ytrain)\n",
    "pred = sgd.predict(xtest)\n",
    "print(r2_score(ytest, pred))\n",
    "print(sgd.coef_, sgd.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf2eba3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4625175538679491\n",
      "[  34.53481516 -290.8297132   482.42486674  368.06264241 -850.95867099\n",
      "  500.43286501  179.42011869  270.53452807  759.17644526   37.48099272] 151.10138570715063\n"
     ]
    }
   ],
   "source": [
    "# we can also use the Ridge class to use GD to fit the model\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "r = Ridge(alpha = 0.001, solver = 'saga') # solver saga uses stochastic GD\n",
    "r.fit(xtrain, ytrain)\n",
    "pred2 = r.predict(xtest)\n",
    "print(r2_score(ytest, pred2))\n",
    "print(r.coef_, r.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ea8ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our own class implementing GD to solve Ridge Regression\n",
    "\n",
    "class MeraRidge :\n",
    "    \n",
    "    def __init__(self, alpha = 0.001, learning_rate = 0.001, epochs = 100):\n",
    "        self.alpha = alpha\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fit(self, xtrain, ytrain):\n",
    "        # here in GD , we have to initially take some coeff values so we'll create weights vector with forst value for intercept and rest for other coef\n",
    "        self.coef_ = np.ones(xtrain.shape[1])\n",
    "        self.intercept_ = 0\n",
    "        weights = np.insert(self.coef_, 0, self.intercept_) # set 1st col of coef to be the intercept term for the model\n",
    "        \n",
    "        xtrain = np.insert(xtrain, 0, 1, axis = 1) # making our xtrain column as legible for being multiplied by weights\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            derivative = np.dot(xtrain.T, xtrain).dot(weights) - np.dot(ytrain.T, xtrain) + self.alpha*weights\n",
    "            weights = weights - self.learning_rate*derivative\n",
    "        \n",
    "        self.coef_ = weights[1:]\n",
    "        self.intercept_ = weights[0]\n",
    "        \n",
    "        \n",
    "    def predict(self, xtest):\n",
    "        #print((np.dot(xtest, self.coef_.reshape(-1,1)) + self.intercept_).shape)\n",
    "        return np.dot(xtest, self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54f0085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47007144598130957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150.89079192829706,\n",
       " array([  47.45988234, -201.25276553,  426.38647344,  308.29371495,\n",
       "         -21.75009137,  -84.56646679, -183.71317602,  143.73095353,\n",
       "         379.48066772,   95.75667701]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we noted that increasing lr value is causing certain errors related to overflow in our class implementation\n",
    "# also note that we ae applying little bit of regulaization only\n",
    "reg = MeraRidge(alpha = 0.1,epochs = 1000, learning_rate = 0.0026)\n",
    "reg.fit(xtrain, ytrain)\n",
    "preds = reg.predict(xtest)\n",
    "print(r2_score(ytest,preds))\n",
    "reg.intercept_, reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6a335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
